% This file was created with Citavi 6.8.0.0

@proceedings{.1994,
 year = "1994",
 title = "{NIPS}"
}


@proceedings{.1995,
 year = "1995"
}


@proceedings{.2001,
 year = "2001",
 title = "{Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001}",
 publisher = "{IEEE Comput. Soc}",
 isbn = "0-7695-1272-0"
}


@proceedings{.2001b,
 year = "2001",
 title = "{2001 IEEE Computer Society Conference on Computer Vision and Pattern  Recognition (CVPR 2001), with CD-ROM, 8-14 December 2001, Kauai,  HI, USA}",
 publisher = "{IEEE Computer Society}"
}


@proceedings{.2005,
 year = "2005",
 title = "{10th IEEE International Conference on Computer Vision (ICCV 2005),  17-20 October 2005, Beijing, China}",
 publisher = "{IEEE Computer Society}"
}


@proceedings{.2007,
 year = "2007",
 title = "{Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2}",
 publisher = "IEEE",
 isbn = "0-7695-2822-8"
}


@proceedings{.2009,
 year = "2009",
 title = "{2009 Eighth International Symposium on Natural Language Processing}",
 publisher = "IEEE",
 isbn = "978-1-4244-4138-9"
}


@proceedings{.2009b,
 year = "2009",
 title = "{2009 IEEE Computer Society Conference on Computer Vision and Pattern  Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA}",
 publisher = "{IEEE Computer Society}"
}


@proceedings{.2011,
 year = "2011",
 title = "{2011 International Conference on Document Analysis and Recognition}",
 publisher = "IEEE",
 isbn = "978-1-4577-1350-7"
}


@proceedings{.2013,
 year = "2013",
 title = "{2013 IEEE Symposium on Computers and Communications (ISCC)}",
 publisher = "IEEE",
 isbn = "978-1-4799-3755-4"
}


@proceedings{.2013b,
 year = "2013",
 title = "{2013 12th International Conference on Document Analysis and Recognition}",
 publisher = "IEEE",
 isbn = "978-0-7695-4999-6"
}


@proceedings{.2013c,
 year = "2013",
 title = "{12th International Conference on Document Analysis and Recognition,  ICDAR 2013, Washington, DC, USA, August 25-28, 2013}",
 publisher = "{IEEE Computer Society}"
}


@proceedings{.2015,
 year = "2015",
 title = "{2015 13th International Conference on Document Analysis and Recognition (ICDAR)}",
 publisher = "IEEE",
 isbn = "978-1-4799-1805-8"
}


@proceedings{.2015b,
 year = "2015",
 title = "{2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)}",
 publisher = "IEEE",
 isbn = "978-1-4673-8564-0"
}


@proceedings{.2015c,
 year = "2015",
 title = "{2015 IEEE International Conference on Computer Vision, ICCV 2015,  Santiago, Chile, December 7-13, 2015}",
 publisher = "{IEEE Computer Society}"
}


@proceedings{.2016,
 year = "2016",
 title = "{2016 12th IAPR Workshop on Document Analysis Systems (DAS)}",
 publisher = "IEEE",
 isbn = "978-1-5090-1792-8"
}


@proceedings{.2017,
 year = "2017",
 title = "{2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)}",
 publisher = "IEEE",
 isbn = "978-1-5386-3586-5"
}


@proceedings{.2018,
 year = "2018",
 title = "{2018 IEEE Conference on Computer Vision and Pattern Recognition,  CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018}",
 publisher = "{IEEE Computer Society}"
}


@misc{Abadi.2015,
 author = "Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang",
 year = "2015",
 title = "{TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems}",
 url = "https://www.tensorflow.org/"
}


@article{Agnihotri.2020,
 author = "Agnihotri, Apoorv and Batra, Nipun",
 year = "2020",
 title = "{Exploring Bayesian Optimization}",
 journal = "{Distill}"
}


@inproceedings{Ahmed.2013,
 author = "Ahmed, Sheraz and Shafait, Faisal and Liwicki, Marcus and Dengel, Andreas",
 title = "{A Generic Method for Stamp Segmentation Using Part-Based Features}",
 pages = "708--712",
 publisher = "{IEEE Computer Society}",
 booktitle = "{12th International Conference on Document Analysis and Recognition,  ICDAR 2013, Washington, DC, USA, August 25-28, 2013}",
 year = "2013"
}


@phdthesis{Ahmed.2016,
 abstract = "A part-based feature refers to local properties of a part/local-area/patch of an image [68]. These properties can be based on change in intensity, color, texture, gradient, etc. Local features are usually distinct from their neighborhood. For example, corners of an object, usually exhibits different properties than the object itself.

To learn the properties of different types of information it is important to first have some sample images for each of the target information classes. Once these properties are learnt, feature banks containing features for each type information are generated. Later, the prediction uses these feature banks to segment different types of information available in previously unseen documents.",
 author = "Ahmed, Sheraz",
 year = "2016",
 title = "{A Generic Framework for Information Segmentation in Document Images: A part-based Approach}",
 address = "Kaiserslautern",
 school = "{Technische Universität Kaiserslautern}",
 type = "{Dissertation}"
}


@misc{Alber.2018,
 abstract = "In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and pre- dictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short- coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the- box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.",
 author = "Alber, Maximilian and Lapuschkin, Sebastian and Seegerer, Philipp and Hägele, Miriam and Schütt, Kristof T. and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert and Dähne, Sven and Kindermans, Pieter-Jan",
 date = "2018",
 title = "{iNNvestigate neural networks!}",
 url = "http://arxiv.org/pdf/1808.04260v1",
 file = "https://arxiv.org/pdf/1808.04260v1.pdf",
 file = "http://arxiv.org/abs/1808.04260v1"
}


@article{Bach.2015,
 abstract = "Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.",
 author = "Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech",
 year = "2015",
 title = "{On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation}",
 pages = "e0130140",
 volume = "10",
 number = "7",
 journal = "{PloS one}"
}


@proceedings{Bartlett.2012,
 year = "2012",
 title = "{Advances in Neural Information Processing Systems 25: 26th Annual  Conference on Neural Information Processing Systems 2012. Proceedings  of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States}",
 editor = "Bartlett, Peter L. and Pereira, Fernando C. N. and Burges, Christopher J. C. and Bottou, Léon and Weinberger, Kilian Q."
}


@proceedings{Bengio.2015,
 year = "2015",
 title = "{3rd International Conference on Learning Representations, ICLR 2015,  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings}",
 editor = "Bengio, Yoshua and Lecun, Yann"
}


@misc{Bhalgat.2016,
 abstract = "Document digitization is becoming increasingly crucial. In this work, we propose a shape based approach for automatic stamp verification/detection in document images using an unsupervised feature learning. Given a small set of training images, our algorithm learns an appropriate shape representation using an unsupervised clustering. Experimental results demonstrate the effectiveness of our framework in challenging scenarios.",
 author = "Bhalgat, Yash and Kulkarni, Mandar and Karande, Shirish and Lodha, Sachin",
 date = "2016",
 title = "{Stamp processing with examplar features}"
}


@article{Bradski.2000,
 author = "Bradski, G.",
 year = "2000",
 title = "{The OpenCV Library}",
 journal = "{Dr. Dobb’s Journal of Software Tools}"
}


@misc{Chen.2017,
 abstract = "We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete and APX-hard. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.",
 author = "Chen, Yining and Gilroy, Sorcha and Maletti, Andreas and May, Jonathan and Knight, Kevin",
 date = "2017",
 title = "{Recurrent Neural Networks as Weighted Language Recognizers}",
 file = "https://arxiv.org/pdf/1711.05408v2.pdf"
}


@article{Comrie.1997,
 author = "Comrie, Andrew C.",
 year = "1997",
 title = "{Comparing Neural Networks and Regression Models for Ozone Forecasting}",
 pages = "653--663",
 volume = "47",
 number = "6",
 issn = "1096-2247",
 journal = "{Journal of the Air {\&} Waste Management Association}",
 file = "Comparing Neural Networks and Regression Models for Ozone Forecasting:C\:\\Users\\zietl\\Documents\\_EIGENE_DOKUMENTE\\_Uni\\Transferleistungen\\TF2b\\Citavi\\TF3TK\\Citavi Attachments\\Comparing Neural Networks and Regression Models for Ozone Forecasting.pdf:pdf"
}


@misc{Cui.2019,
 abstract = "With the widespread applications of deep convolutional neural networks (DCNNs), it becomes increasingly important for DCNNs not only to make accurate predictions but also to explain how they make their decisions. In this work, we propose a CHannel-wise disentangled InterPretation (CHIP) model to give the visual interpretation to the predictions of DCNNs. The proposed model distills the class-discriminative importance of channels in networks by utilizing the sparse regularization. Here, we first introduce the network perturbation technique to learn the model. The proposed model is capable to not only distill the global perspective knowledge from networks but also present the class-discriminative visual interpretation for specific predictions of networks. It is noteworthy that the proposed model is able to interpret different layers of networks without re-training. By combining the distilled interpretation knowledge in different layers, we further propose the Refined CHIP visual interpretation that is both high-resolution and class-discriminative. Experimental results on the standard dataset demonstrate that the proposed model provides promising visual interpretation for the predictions of networks in image classification task compared with existing visual interpretation methods. Besides, the proposed method outperforms related approaches in the application of ILSVRC 2015 weakly-supervised localization task.",
 author = "Cui, Xinrui and Wang, Dan and Wang, Z. Jane",
 date = "2019",
 title = "{CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional  Neural Networks}",
 file = "CHIP - Channel-wise Disentangled Interpretation of Deep Convolutional  Neural Networks:C\:\\Users\\zietl\\Documents\\_EIGENE_DOKUMENTE\\_Uni\\Transferleistungen\\TF2b\\Citavi\\TF3TK\\Citavi Attachments\\CHIP - Channel-wise Disentangled Interpretation of Deep Convolutional  Neural Networks.pdf:pdf"
}


@article{Day.2017,
 author = "Day, Oscar and Khoshgoftaar, Taghi M.",
 year = "2017",
 title = "{A survey on heterogeneous transfer learning}",
 pages = "29",
 volume = "4",
 journal = "{J. Big Data}"
}


@inproceedings{Deng.2009,
 author = "Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Li, Fei-Fei",
 title = "{ImageNet: A large-scale hierarchical image database}",
 pages = "248--255",
 publisher = "{IEEE Computer Society}",
 booktitle = "{2009 IEEE Computer Society Conference on Computer Vision and Pattern  Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA}",
 year = "2009"
}


@inproceedings{Dey.2015,
 author = "Dey, Soumyadeep and Mukherjee, Jayanta and Sural, Shamik",
 title = "{Stamp and logo detection from document images by finding outliers}",
 pages = "1--4",
 publisher = "IEEE",
 isbn = "978-1-4673-8564-0",
 booktitle = "{2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)}",
 year = "2015",
 file = "http://ieeexplore.ieee.org/document/7489947/",
 file = "Stamp and Logo Detection from Document Images by Finding Outliers:C\:\\Users\\zietl\\Documents\\_EIGENE_DOKUMENTE\\_Uni\\Transferleistungen\\TF2b\\Citavi\\TF3TK\\Citavi Attachments\\Stamp and Logo Detection from Document Images by Finding Outliers.pdf:pdf"
}


@inproceedings{Dey.2016,
 author = "Dey, Soumyadeep and Mukhopadhyay, Jayanta and Sural, Shamik",
 title = "{Removal of Gray Rubber Stamps}",
 pages = "210--214",
 publisher = "IEEE",
 isbn = "978-1-5090-1792-8",
 booktitle = "{2016 12th IAPR Workshop on Document Analysis Systems (DAS)}",
 year = "2016",
 file = "http://ieeexplore.ieee.org/document/7490119/",
 file = "Removal of Gray Rubber Stamps:C\:\\Users\\zietl\\Documents\\_EIGENE_DOKUMENTE\\_Uni\\Transferleistungen\\TF2b\\Citavi\\TF3TK\\Citavi Attachments\\Removal of Gray Rubber Stamps.pdf:pdf"
}


@article{Duda.1972,
 author = "Duda, Richard O. and Hart, Peter E.",
 year = "1972",
 title = "{Use of the Hough Transformation to Detect Lines and Curves in Pictures}",
 pages = "11--15",
 volume = "15",
 number = "1",
 journal = "{Commun. ACM}"
}


@incollection{Forczmanski.2010,
 author = "Forczmański, Paweł and Frejlichowski, Dariusz",
 title = "{Robust Stamps Detection and Classification by Means of General Shape Analysis}",
 pages = "360--367",
 volume = "6374",
 publisher = "{Springer Berlin Heidelberg}",
 isbn = "978-3-642-15909-1",
 series = "{Lecture Notes in Computer Science}",
 editor = "Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Bolc, Leonard and Tadeusiewicz, Ryszard and Chmielewski, Leszek J. and Wojciechowski, Konrad",
 booktitle = "{Computer Vision and Graphics}",
 year = "2010",
 address = "Berlin, Heidelberg"
}


@article{Forczmanski.2015,
 abstract = "Interesting grasp of data charecteristics",
 author = "Forczmański, Paweł and Markiewicz, Andrzej",
 year = "2015",
 title = "{Stamps Detection and Classification Using Simple Features Ensemble}",
 pages = "1--15",
 volume = "2015",
 number = "9",
 issn = "1024-123X",
 journal = "{Mathematical Problems in Engineering}"
}


@article{Forczmanski.2016,
 author = "Forczmański, Paweł and Markiewicz, Andrzej",
 year = "2016",
 title = "{Two-stage approach to extracting visual objects from paper documents}",
 pages = "1243--1257",
 volume = "27",
 number = "8",
 issn = "0932-8092",
 journal = "{Machine Vision and Applications}"
}


@inproceedings{Freund.1995,
 author = "Freund, Yoav and Schapire, Robert E.",
 title = "{A decision-theoretic generalization of on-line learning and an application  to boosting}",
 pages = "23--37",
 volume = "904",
 publisher = "Springer",
 series = "{Lecture Notes in Computer Science}",
 editor = "Vitányi, Paul M. B.",
 booktitle = "{Computational Learning Theory, Second European Conference, EuroCOLT  ’95, Barcelona, Spain, March 13-15, 1995, Proceedings}",
 year = "1995"
}


@misc{Fu.20170123,
 abstract = "The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with {\$}513 \times 513{\$} input achieves 81.5{\%} mAP on VOC2007 test, 80.0{\%} mAP on VOC2012 test, and 33.2{\%} mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset.",
 author = "Fu, Cheng-Yang and Liu, Wei and Ranga, Ananth and Tyagi, Ambrish and Berg, Alexander C.",
 date = "2017",
 title = "{DSSD : Deconvolutional Single Shot Detector}"
}


@article{Fukunaga.1975,
 author = "Fukunaga, Keinosuke and Hostetler, Larry D.",
 year = "1975",
 title = "{The estimation of the gradient of a density function, with applications  in pattern recognition}",
 pages = "32--40",
 volume = "21",
 number = "1",
 journal = "{IEEE Trans. Inf. Theory}"
}


@book{Garain.2015,
 year = "2015",
 title = "{Computational Forensics}",
 address = "Cham",
 publisher = "{Springer International Publishing}",
 isbn = "978-3-319-20124-5",
 series = "{Lecture Notes in Computer Science}",
 editor = "Garain, Utpal and Shafait, Faisal"
}


@inproceedings{Girshick.2015,
 author = "Girshick, Ross B.",
 title = "{Fast R-CNN}",
 pages = "1440--1448",
 publisher = "{IEEE Computer Society}",
 booktitle = "{2015 IEEE International Conference on Computer Vision, ICCV 2015,  Santiago, Chile, December 7-13, 2015}",
 year = "2015"
}


@book{Goodfellow.2016,
 author = "Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron",
 year = "2016",
 title = "{Deep Learning}",
 publisher = "{MIT Press}"
}


@book{Grohs.2021,
 year = "2021",
 title = "{Mathematical Aspects of Deep Learning}",
 publisher = "{Cambridge University Press}",
 editor = "Grohs, Philipp and Kutyniok, Gitta"
}


@article{Gu.,
 abstract = "A number of backpropagation-based approaches such as DeConvNets, vanilla Gradient Visualization and Guided Backpropagation have been proposed to better understand individual decisions of deep convolutional neural networks. The saliency maps produced by them are proven to be non-discriminative. Recently, the Layer-wise Relevance Propagation (LRP) approach was proposed to explain the classification decisions of rectifier neural networks. In this work, we evaluate the discriminativeness of the generated explanations and analyze the theoretical foundation of LRP, i.e. Deep Taylor Decomposition. The experiments and analysis conclude that the explanations generated by LRP are not class-discriminative. Based on LRP, we propose Contrastive Layer-wise Relevance Propagation (CLRP), which is capable of producing instance-specific, class-discriminative, pixel-wise explanations. In the experiments, we use the CLRP to explain the decisions and understand the difference between neurons in individual classification decisions. We also evaluate the explanations quantitatively with a Pointing Game and an ablation study. Both qualitative and quantitative evaluations show that the CLRP generates better explanations than the LRP.",
 author = "Gu, Jindong and Yang, Yinchong and Tresp, Volker",
 title = "{Understanding Individual Decisions of CNNs via Contrastive  Backpropagation}"
}


@phdthesis{Guancong.2011,
 author = "Guancong, Li",
 year = "2011",
 title = "{Postage Postage Postage Postage stamp recognition recognition recognition recognition using image processing processing processing}",
 address = "Gävle, Sweden",
 urldate = "08.04.2019",
 publisher = "Faculty of Engineering and Sustainable Development",
 school = "{University of Gävle}",
 type = "{Bachelor Thesis}"
}


@article{Hassanzadeh.2011,
 author = "Hassanzadeh, Sina and Pourghassem, Hossein",
 year = "2011",
 title = "{A Novel Logo Detection and Recognition Framework for Separated Part Logos in Document Images}",
 volume = "5",
 journal = "{Australian Journal of Basic and Applied Sciences}"
}


@book{Hastie.2001,
 abstract = "During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.",
 author = "Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.",
 year = "2001",
 title = "{The Elements of Statistical Learning: Data mining, inference, and prediction}",
 address = "New York",
 edition = "2",
 publisher = "Springer",
 isbn = "978-0-387-21606-5",
 series = "{Springer series in statistics}"
}


@misc{He.2017,
 abstract = "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron",
 author = "He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross",
 date = "2017",
 title = "{Mask R-CNN}"
}


@inproceedings{Horne.1994,
 author = "Horne, Bill G. and Giles, C. Lee",
 title = "{An experimental comparison of recurrent neural networks}",
 booktitle = "{NIPS}",
 year = "1994"
}


@article{Hotelling.1933,
 author = "Hotelling, Harold",
 year = "1933",
 title = "{Analysis of a complex of statistical variables into principal components}",
 pages = "417--441",
 volume = "24",
 number = "6",
 journal = "{Journal of educational psychology}"
}


@article{Howard.2017,
 author = "Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig",
 year = "2017",
 title = "{MobileNets: Efficient Convolutional Neural Networks for Mobile Vision  Applications}",
 volume = "abs/1704.04861",
 journal = "{CoRR}",
 file = "http://arxiv.org/abs/1704.04861"
}


@book{Hutchison.2010,
 year = "2010",
 title = "{Computer Vision and Graphics}",
 address = "Berlin, Heidelberg",
 publisher = "{Springer Berlin Heidelberg}",
 isbn = "978-3-642-15909-1",
 series = "{Lecture Notes in Computer Science}",
 editor = "Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and {Pandu Rangan}, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Bolc, Leonard and Tadeusiewicz, Ryszard and Chmielewski, Leszek J. and Wojciechowski, Konrad"
}


@article{James.2011,
 author = "James, Craig A. and Weininger, D. and Delany, J.",
 year = "2011",
 title = "{Daylight Theory Manual. Daylight Chemical Information Systems}",
 journal = "{Inc., Irvine, CA}"
}


@book{James.2017,
 author = "James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert",
 year = "2017",
 title = "{An introduction to statistical learning: With applications in R}",
 address = "New York and Heidelberg and Dordrecht and London",
 edition = "Corrected at 8th printing",
 publisher = "Springer",
 isbn = "1461471370",
 series = "{Springer texts in statistics}"
}


@inproceedings{Jeatrakul.2009,
 author = "Jeatrakul, P. and Wong, K. W.",
 title = "{Comparing the performance of different neural networks for binary classification problems}",
 pages = "111--115",
 publisher = "IEEE",
 isbn = "978-1-4244-4138-9",
 booktitle = "{2009 Eighth International Symposium on Natural Language Processing}",
 year = "2009"
}


@misc{Karpathy.2015,
 abstract = "Course materials and notes for Stanford class CS231n: Convolutional Neural Networks for Visual Recognition.",
 author = "Karpathy, Andrej",
 year = "2015",
 title = "{CS231n Convolutional Neural Networks for Visual Recognition}",
 url = "https://cs231n.github.io/convolutional-networks/",
 urldate = "2021-01-21"
}


@misc{KevinShen.2018,
 author = "{Kevin Shen}",
 editor = "{Kevin Shen}",
 year = "2018",
 title = "{Effect of batch size on training dynamics}",
 url = "https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e",
 urldate = "15.04.2019"
}


@article{Krishnamoorthy.1993,
 author = "Krishnamoorthy, M. and Nagy, G. and Seth, S. and Viswanathan, M.",
 year = "1993",
 title = "{Syntactic segmentation and labeling of digitized pages from technical journals}",
 pages = "737--747",
 volume = "15",
 number = "7",
 issn = "01628828",
 journal = "{IEEE Transactions on Pattern Analysis and Machine Intelligence}"
}


@inproceedings{Krizhevsky.2012,
 author = "Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.",
 title = "{ImageNet Classification with Deep Convolutional Neural Networks}",
 pages = "1106--1114",
 editor = "Bartlett, Peter L. and Pereira, Fernando C. N. and Burges, Christopher J. C. and Bottou, Léon and Weinberger, Kilian Q.",
 booktitle = "{Advances in Neural Information Processing Systems 25: 26th Annual  Conference on Neural Information Processing Systems 2012. Proceedings  of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States}",
 year = "2012",
 file = "https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html"
}


@misc{Lapuschkin.2017,
 abstract = "Recently, deep neural networks have demonstrated excellent performances in recognizing the age and gender on human face images. However, these models were applied in a black-box manner with no information provided about which facial features are actually used for prediction and how these features depend on image preprocessing, model initialization and architecture choice. We present a study investigating these different effects.  In detail, our work compares four popular neural network architectures, studies the effect of pretraining, evaluates the robustness of the considered alignment preprocessings via cross-method test set swapping and intuitively visualizes the model's prediction strategies in given preprocessing conditions using the recent Layer-wise Relevance Propagation (LRP) algorithm. Our evaluations on the challenging Adience benchmark show that suitable parameter initialization leads to a holistic perception of the input, compensating artefactual data representations. With a combination of simple preprocessing steps, we reach state of the art performance in gender recognition.",
 author = "Lapuschkin, Sebastian and Binder, Alexander and Müller, Klaus-Robert and Samek, Wojciech",
 date = "2017",
 title = "{Understanding and Comparing Deep Neural Networks for Age and Gender  Classification}"
}


@inproceedings{Lecun.1995,
 author = "Lecun, Yann and Jackel, Larry and Bottou, L. and Brunot, A. and Cortes, Corinna and Denker, John and Drucker, Harris and Guyon, Isabelle and Muller, Urs and Sackinger, E. and Simard, Patrice and Vapnik, V.",
 title = "{Comparison of learning algorithms for handwritten digit recognition}",
 year = "1995"
}


@proceedings{Leibe.2016,
 year = "2016",
 title = "{Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam,  The Netherlands, October 11-14, 2016, Proceedings, Part I}",
 publisher = "Springer",
 series = "{Lecture Notes in Computer Science}",
 editor = "Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max"
}


@misc{LeiMao.2019,
 author = "{Lei Mao}",
 year = "2019",
 title = "{Bounding Box Encoding and Decoding in Object Detection}",
 url = "leimao.github.io/blog/Bounding-Box-Encoding-Decoding/",
 urldate = "16.04.2019"
}


@inproceedings{Liu.2016,
 author = "Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott E. and Fu, Cheng-Yang and Berg, Alexander C.",
 title = "{SSD: Single Shot MultiBox Detector}",
 pages = "21--37",
 volume = "9905",
 publisher = "Springer",
 series = "{Lecture Notes in Computer Science}",
 editor = "Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max",
 booktitle = "{Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam,  The Netherlands, October 11-14, 2016, Proceedings, Part I}",
 year = "2016"
}


@article{Liu.2020,
 author = "Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul W. and Chen, Jie and Liu, Xinwang and Pietikäinen, Matti",
 year = "2020",
 title = "{Deep Learning for Generic Object Detection: A Survey}",
 pages = "261--318",
 volume = "128",
 number = "2",
 journal = "{Int. J. Comput. Vis.}"
}


@incollection{Marban.2021,
 author = "Marban, Arturo and Srinivasan, Vignesh and Samek, Wojciech and Fernández, Josep and Casals, Alicia",
 title = "{Explaining the Decisions of Convolutional and Recurrent Neural Networks}",
 publisher = "{Cambridge University Press}",
 editor = "Grohs, Philipp and Kutyniok, Gitta",
 booktitle = "{Mathematical Aspects of Deep Learning}",
 year = "2021"
}


@proceedings{Metaxas.2011,
 year = "2011",
 title = "{IEEE International Conference on Computer Vision, ICCV 2011, Barcelona,  Spain, November 6-13, 2011}",
 publisher = "{IEEE Computer Society}",
 editor = "Metaxas, Dimitris N. and Quan, Long and Sanfeliu, Alberto and {van Gool}, Luc"
}


@inproceedings{Micenkova.2011,
 abstract = "Dataset:

D-Star:
For evaluation of D-StaR, we used a publicly available stamp detection and verification dataset1 [4]. This dataset contains 400 document images scanned at 200, 300, and 600 dpi resolution. The scanned document images contain printed text, stamps (textual and non-textual), logos, and signatures. The dataset contains stamps of varying sizes, shapes, and colors. 341 (out of 400) scanned document images contain single or multiple stamps and remaining 59 images have no stamps. Out of 341 scanned document images, 80 contain black stamps, and remaining 241 contain colored stamps. In 55 scanned document images stamps are overlapped with text, logos, and/ or signatures. For every scanned image, there are two ground truth images available; one containing the pixel level information and the other containing the bounding box information for each stamp. So, this dataset can be used for both region classification and pixel level evaluation.

A generic framework:
The presented method is evaluated on a publicly available dataset (StaVer1) for stamp
detection and verication [19]. This dataset contains 400 scanned document images. Out
of these 400 documents, 80 documents contain black stamps whereas the remaining 320
documents contain colored stamps. All of these document images are available in 200,
300, and 600 dpi. For each image, two di
erent types of ground truths are available.
One contains the pixel level ground truth, which means all of the pixels which belong to
stamps are marked in the image. The other ground truth format contains bounding box
information for each stamp. Hence, this dataset can be used for both pixel level as well
as patch level evaluation of stamp detection. In addition, it contains di
erent types of
stamps ranging from rectangular, oval, to irregular shaped, and most importantly, textual
stamps.

Stamp and logo detect from outlier:
The proposed approach is applied on publicly available
StampVer dataset [8]. The dataset comprises of stamps and
logos in different shapes, sizes and color. Both textual and
non-textual stamps with different orientation are present in the
dataset. There are 400 images in the dataset, and all the images
are available in 200, 300, and 600 dpi resolutions. Out of 400
images, 59 images contain no stamp region, and in the remaining
341 images, 261 contain only color stamps, and the rest 80
contain mixture of black and color stamps. Further, StampVer
dataset contains 55 images with overlapped stamp regions, out
of which 52 images are with color stamps. Groundtruth for
only stamp regions is available for the public dataset [8]. We
have generated groundtruth by annotating all the components
of 344 images with non-overlapping components for 300 dpi
resolution images from the StampVer dataset. The annotated
dataset is available online1. We use this groundtruth as well
as publicly available stamp groundtruth [8] for evaluation of
the algorithm for stamp and logo classification of images with
300 dpi resolution.",
 author = "Micenková, Barbora and {van Beusekom}, Joost",
 title = "{Stamp Detection in Color Document Images}",
 pages = "1125--1129",
 publisher = "IEEE",
 isbn = "978-1-4577-1350-7",
 booktitle = "{2011 International Conference on Document Analysis and Recognition}",
 year = "2011"
}


@incollection{Micenkova.2015,
 abstract = "Stamps, along with signatures, can be considered as the most widely used extrinsic security feature in paper documents. In contrast to signatures, however, for stamps little work has been done to automatically verify their authenticity. 

In this paper, an approach for verication of color stamps is presented. We focus on photocopied stamps as non-genuine stamps. Our previously presented stamp detection method is improved and extended to verify that the stamp is genuine and not a copy. Using a variety of features, a classier is trained that allows successful separation between genuine stamps and copied stamps. 

Sensitivity and specicity of up to 95{\%} could be obtained on a data set that is publicly available.",
 author = "Micenková, Barbora and {van Beusekom}, Joost and Shafait, Faisal",
 title = "{Stamp Verification for Automated Document Authentication}",
 pages = "117--129",
 volume = "8915",
 publisher = "{Springer International Publishing}",
 isbn = "978-3-319-20124-5",
 series = "{Lecture Notes in Computer Science}",
 editor = "Garain, Utpal and Shafait, Faisal",
 booktitle = "{Computational Forensics}",
 year = "2015",
 address = "Cham"
}


@article{Montavon.2017,
 abstract = "Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems, e.g., image classification, natural language processing or human action recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method is based on deep Taylor decomposition and efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.",
 author = "Montavon, Grégoire and Bach, Sebastian and Binder, Alexander and Samek, Wojciech and Müller, Klaus-Robert",
 year = "2017",
 title = "{Explaining NonLinear Classification Decisions with Deep Taylor  Decomposition}",
 pages = "211--222",
 volume = "65",
 issn = "00313203",
 journal = "{Pattern Recognition}"
}


@book{Murphy.2012,
 author = "Murphy, Kevin P.",
 year = "2012",
 title = "{Machine learning - a probabilistic perspective}",
 publisher = "{MIT Press}",
 isbn = "0262018020",
 series = "{Adaptive computation and machine learning series}"
}


@inproceedings{Nandedkar.2015,
 author = "Nandedkar, Amit Vijay and Mukhopadhyay, Jayanta and Sural, Shamik",
 title = "{Text-graphics separation to detect logo and stamp from color document images: A spectral approach}",
 pages = "571--575",
 publisher = "IEEE",
 isbn = "978-1-4799-1805-8",
 booktitle = "{2015 13th International Conference on Document Analysis and Recognition (ICDAR)}",
 year = "2015",
 file = "http://ieeexplore.ieee.org/document/7333826/"
}


@inproceedings{Nandedkar.2015b,
 author = "Nandedkar, Amit V. and Mukherjee, Jayanta and Sural, Shamik",
 title = "{A spectral filtering based deep learning for detection of logo and stamp}",
 pages = "1--4",
 publisher = "IEEE",
 isbn = "978-1-4673-8564-0",
 booktitle = "{2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)}",
 year = "2015"
}


@book{Nielsen.2015,
 author = "Nielsen, Michael A.",
 year = "2015",
 title = "{Neural Networks and Deep Learning}",
 publisher = "{Determination Press}"
}


@misc{Padilla.2019,
 author = "Padilla, Rafael",
 year = "2019",
 title = "{Object Detection Metrics}",
 publisher = "GitHub",
 journal = "{GitHub repository}"
}


@inproceedings{Petej.2013,
 author = "Petej, Pjero and Gotovac, Sven",
 title = "{Comparison of stamp classification using SVM and random ferns}",
 pages = "000850--000854",
 publisher = "IEEE",
 isbn = "978-1-4799-3755-4",
 booktitle = "{2013 IEEE Symposium on Computers and Communications (ISCC)}",
 year = "2013"
}


@misc{Redmon.2015,
 abstract = "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",
 author = "Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali",
 date = "2015",
 title = "{You Only Look Once: Unified, Real-Time Object Detection}"
}


@misc{Redmon.2016,
 author = "Redmon, Joseph",
 year = "2016",
 title = "{Darknet: Open Source Neural Networks in C}"
}


@misc{Redmon.2016b,
 abstract = "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.",
 author = "Redmon, Joseph and Farhadi, Ali",
 date = "2016",
 title = "{YOLO9000: Better, Faster, Stronger}"
}


@misc{Redmon.2018,
 abstract = "We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/",
 author = "Redmon, Joseph and Farhadi, Ali",
 date = "2018",
 title = "{YOLOv3: An Incremental Improvement}",
 url = "http://arxiv.org/pdf/1804.02767v1",
 file = "http://arxiv.org/abs/1804.02767v1",
 file = "https://arxiv.org/pdf/1804.02767v1.pdf"
}


@misc{Ren.2015,
 abstract = "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
 author = "Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian",
 date = "2015",
 title = "{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal  Networks}"
}


@inproceedings{Rosten.2005,
 author = "Rosten, Edward and Drummond, Tom",
 title = "{Fusing Points and Lines for High Performance Tracking}",
 pages = "1508--1515",
 publisher = "{IEEE Computer Society}",
 booktitle = "{10th IEEE International Conference on Computer Vision (ICCV 2005),  17-20 October 2005, Beijing, China}",
 year = "2005"
}


@inproceedings{Rublee.2011,
 author = "Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary R.",
 title = "{ORB: An efficient alternative to SIFT or SURF}",
 pages = "2564--2571",
 publisher = "{IEEE Computer Society}",
 editor = "Metaxas, Dimitris N. and Quan, Long and Sanfeliu, Alberto and {van Gool}, Luc",
 booktitle = "{IEEE International Conference on Computer Vision, ICCV 2011, Barcelona,  Spain, November 6-13, 2011}",
 year = "2011"
}


@misc{Selvaraju.2016,
 abstract = "We propose a technique for producing {\textquotedbl}visual explanations{\textquotedbl} for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any architectural changes or re-training. We combine GradCAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on weakly-supervised localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we conduct human studies to measure if GradCAM explanations help users establish trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a {\textquotedbl}stronger{\textquotedbl} deep network from a {\textquotedbl}weaker{\textquotedbl} one. Our code is available at https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.",
 author = "Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv",
 date = "2016",
 title = "{Grad-CAM: Visual Explanations from Deep Networks via Gradient-based  Localization}"
}


@misc{Shiebler.2017,
 author = "Shiebler, Dan",
 year = "2017",
 title = "{Understanding Neural Networks with Layerwise Relevance Propagation and Deep Taylor Series}",
 url = "http://danshiebler.com/2017-04-16-deep-taylor-lrp/"
}


@article{Siegelmann.1995,
 author = "Siegelmann, H. T. and Sontag, E. D.",
 year = "1995",
 title = "{On the Computational Power of Neural Nets}",
 pages = "132--150",
 volume = "50",
 number = "1",
 issn = "00220000",
 journal = "{Journal of Computer and System Sciences}"
}


@inproceedings{Simonyan.2015,
 author = "Simonyan, Karen and Zisserman, Andrew",
 title = "{Very Deep Convolutional Networks for Large-Scale Image Recognition}",
 editor = "Bengio, Yoshua and Lecun, Yann",
 booktitle = "{3rd International Conference on Learning Representations, ICLR 2015,  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings}",
 year = "2015",
 file = "http://arxiv.org/abs/1409.1556"
}


@proceedings{Singh.2017,
 year = "2017",
 title = "{Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,  February 4-9, 2017, San Francisco, California, USA}",
 publisher = "{AAAI Press}",
 editor = "Singh, Satinder P. and Markovitch, Shaul"
}


@patent{Steinbuch.1963,
 author = "Steinbuch, Karl and Endres, Hermann",
 year = "1963",
 title = "{Postage stamp detecting circuit arrangement}",
 number = "US3087141A"
}


@inproceedings{Szegedy.2017,
 author = "Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.",
 title = "{Inception-v4, Inception-ResNet and the Impact of Residual Connections  on Learning}",
 url = "http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806",
 pages = "4278--4284",
 publisher = "{AAAI Press}",
 editor = "Singh, Satinder P. and Markovitch, Shaul",
 booktitle = "{Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,  February 4-9, 2017, San Francisco, California, USA}",
 year = "2017"
}


@proceedings{Taghva.2006,
 year = "2006",
 title = "{Document Recognition and Retrieval XIII}",
 publisher = "SPIE",
 series = "{SPIE Proceedings}",
 editor = "Taghva, Kazem and Lin, Xiaofan"
}


@article{Tensmeyer.2020,
 author = "Tensmeyer, Chris and Martinez, Tony R.",
 year = "2020",
 title = "{Historical Document Image Binarization: A Review}",
 pages = "173",
 volume = "1",
 number = "3",
 journal = "{SN Comput. Sci.}"
}


@inproceedings{Viola.2001,
 author = "Viola, P. and Jones, M.",
 title = "{Rapid object detection using a boosted cascade of simple features}",
 pages = "I-511-I-518",
 publisher = "{IEEE Comput. Soc}",
 isbn = "0-7695-1272-0",
 booktitle = "{Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001}",
 year = "2001"
}


@proceedings{Vitanyi.1995,
 year = "1995",
 title = "{Computational Learning Theory, Second European Conference, EuroCOLT  ’95, Barcelona, Spain, March 13-15, 1995, Proceedings}",
 publisher = "Springer",
 series = "{Lecture Notes in Computer Science}",
 editor = "Vitányi, Paul M. B."
}


@article{Wang.2016,
 author = "Wang, Fanglin and Qi, Shuhan and Gao, Ge and Zhao, Sicheng and Wang, Xiangyu",
 year = "2016",
 title = "{Logo information recognition in large-scale social media data}",
 pages = "63--73",
 volume = "22",
 number = "1",
 issn = "0942-4962",
 journal = "{Multimedia Systems}"
}


@misc{Weiss.2018,
 abstract = "While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of RNNs with finite precision whose computation time is linear in the input length. Under these limitations, we show that different RNN variants have different computational power. In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU. This is achieved because LSTMs and ReLU-RNNs can easily implement counting behavior. We show empirically that the LSTM does indeed learn to effectively use the counting mechanism.",
 author = "Weiss, Gail and Goldberg, Yoav and Yahav, Eran",
 date = "2018",
 title = "{On the Practical Computational Power of Finite Precision RNNs for  Language Recognition}",
 file = "https://arxiv.org/pdf/1805.04908v1.pdf"
}


@inproceedings{Younas.2017,
 author = "Younas, Junaid and Afzal, Muhammad Zeshan and Malik, Muhammad Imran and Shafait, Faisal and Lukowicz, Paul and Ahmed, Sheraz",
 title = "{D-StaR: A Generic Method for Stamp Segmentation from Document Images}",
 pages = "248--253",
 publisher = "IEEE",
 isbn = "978-1-5386-3586-5",
 booktitle = "{2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)}",
 year = "2017"
}


@misc{Yu.2018,
 abstract = "In this paper we show the similarities and differences of two deep neural networks by comparing the manifolds composed of activation vectors in each fully connected layer of them. The main contribution of this paper includes 1) a new data generating algorithm which is crucial for determining the dimension of manifolds; 2) a systematic strategy to compare manifolds. Especially, we take Riemann curvature and sectional curvature as part of criterion, which can reflect the intrinsic geometric properties of manifolds. Some interesting results and phenomenon are given, which help in specifying the similarities and differences between the features extracted by two networks and demystifying the intrinsic mechanism of deep neural networks.",
 author = "Yu, Tao and Long, Huan and Hopcroft, John E.",
 date = "2018",
 title = "{Curvature-based Comparison of Two Neural Networks}"
}


@misc{Zeiler.2013,
 abstract = "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
 author = "Zeiler, Matthew D. and Fergus, Rob",
 date = "2013",
 title = "{Visualizing and Understanding Convolutional Networks}"
}


@inproceedings{Zhu.2006,
 author = "Zhu, Guangyu and Jaeger, Stefan and Doermann, David",
 title = "{A robust stamp detection framework on degraded documents}",
 pages = "60670B-60670B-9",
 publisher = "SPIE",
 series = "{SPIE Proceedings}",
 editor = "Taghva, Kazem and Lin, Xiaofan",
 booktitle = "{Document Recognition and Retrieval XIII}",
 year = "2006"
}


@inproceedings{Zhu.2007,
 author = "Zhu, G. and Doermann, D.",
 title = "{Automatic Document Logo Detection}",
 pages = "864--868",
 publisher = "IEEE",
 isbn = "0-7695-2822-8",
 booktitle = "{Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2}",
 year = "2007",
 file = "http://ieeexplore.ieee.org/document/4377038/"
}


@inproceedings{Zoph.2018,
 author = "Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and {Le V}, Quoc",
 title = "{Learning Transferable Architectures for Scalable Image Recognition}",
 pages = "8697--8710",
 publisher = "{IEEE Computer Society}",
 booktitle = "{2018 IEEE Conference on Computer Vision and Pattern Recognition,  CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018}",
 year = "2018",
 file = "http://openaccess.thecvf.com/content/_cvpr/_2018/html/Zoph/_Learning/_Transferable/_Architectures/_CVPR/_2018/_paper.html"
}


