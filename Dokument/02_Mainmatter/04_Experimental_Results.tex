\section{Experimental Results}
This chapter gives an overview of the experimental setup, applied metrics
(\cref{sect:metrics}), and dataset (\cref{sect:staver}). Summary and conclusion
are given in \cref{sect:results-and-discussion}.

\subsection{Metrics}\label{sect:metrics}
To evaluate the results achieved by \gls{ssd} (see \cref{sect:methodology}) and
make them comparable to diverging approaches, a set of metrics is introduced in
this section. We consider \nameref{subsect:pixel-wise-metrics}, \nameref{subsect:mAP}
and also introduce \nameref{subsect:normalized-iou}.

\subsubsection{Pixel-wise Metrics}\label{subsect:pixel-wise-metrics}
In this subsubsection we briefly explain pixel-wise metrics such as
\nameref{par:precision-recall}, \nameref{par:f-scores} and \nameref{par:iou}.

\paragraph{Precision \& Recall}\label{par:precision-recall}
To the best of the author's knowledge, the most influential metric used in stamp 
detection is pixel-wise evaluation of the precision and recall
tuple~\cite{Nandedkar.2015b,Younas.2017,Ahmed.2013,Dey.2015,Micenkova.2011, Bhalgat.2016, Micenkova.2015,Nandedkar.2015b}.
Often, recall and precision are used in settings with class imbalance where they
provide a more sensible measure than accuracy. In the stamp detection, every
image pixel can be considered to be from one of two classes, either
\textit{stamp} (i.e.\ positive) or \textit{non-stamp} (i.e.\ negative). In
training and evaluation images, \textit{non-stamp}-pixels greatly outnumber
\textit{stamp}-pixels, thereby showing obvious class imbalance.

A definition of precision and recall is given in~\cite[423]{Goodfellow.2016} as
follows. Precision is the fraction of \glsdisp{tp}{\textit{true} positive (TP)}
pixels over all detected pixels (see \cref{eq:precision}). Recall is the fraction
of \glsdisp{tp}{\textit{true} positive} detections over all \gls{gt} pixels
(see \cref{eq:recall}). This relationship is visually depicted in \cref{fig:visual-precision-recall}.
\begin{align}
    \label{eq:precision}\text{Precision}    &= \frac{TP}{TP + FP}\\
    \label{eq:recall}\text{Recall}          &= \frac{TP}{TP + FN}
\end{align}
Intuitively, precision is the ``ability of a classifier to distinguish a
negative sample from [a] positive one''~\cite{Younas.2017}, while
recall is ``the ability of a classifier to classify all positive samples''~\cite{Younas.2017}.
\begin{figure}
    \center
    \includegraphics[width=.6\textwidth]{Metrics2.png}
    \caption[short={Visual example of pixel-wise recall, precision and \gls{iou}}]
    {Visual example of pixel-wise recall, precision and \gls{iou}, with
    \gls{gt} (red) and detected pixels (blue)}\label{fig:visual-precision-recall}
\end{figure}

\paragraph{F-Scores}\label{par:f-scores}
Precision and recall can then be combined into a harmonic average, which is usually
called the family of \(F\)-Scores~\cite[e.g.][183]{Murphy.2012}. \(F_1\) is given
in~\cite[183]{Murphy.2012} as per \cref{eq:f1}.
\begin{equation}\label{eq:f1}
    F_1=\frac{2*\text{Precision}*\text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}
However, almost all works reviewed in \cref{sect:related-work} give only
precision \& recall and omit F-Scores.

\paragraph{Intersection over Union}\label{par:iou}
\Gls{iou}, also known as the Jaccard-Index or Jaccard Similarity is a metric
closely related to \nameref{par:f-scores} via the Tversky-Index (for details on
this \cite[cf.][Section 6.3 Similarity Measures]{James.2011}). \Gls{iou} is 
given by
\begin{equation}
    \text{IoU}=\frac{TP}{TP + FN + FP}
\end{equation}
Visually, this relation is shown also in \cref{fig:visual-precision-recall}.

\subsubsection{Class-wise Mean Average Precision}\label{subsect:mAP}
\Gls{map} is a widely used metric in object detection that was also adopted in
challenges such as \gls{coco}, \gls{voc}, and \gls{oi}. We define \gls{map} in
context of a multi-classification setting, where \gls{map} represents the
arithmetic mean of \gls{ap} over the set of all classes. Next up we will
redefine confusion matrix-values, then average precision will be explained.

\paragraph{Confusion Matrix}
Unlike in \fref{subsect:pixel-wise-metrics}, confusion matrix-values
(\glspl{tp}, \glspl{fp} \& \glspl{fn}) are attributed not per individual pixel,
but per \gls{bbox}. A predicted \gls{bbox} is considered a \gls{tp}, if its
(pixel-wise) \gls{iou} with a groundtruth \gls{bbox} is greater than some
threshold, \gls{fp} otherwise. Groundtruth \glspl{bbox} without an inferred
\gls{bbox} are considered \gls{fn}.
\par
For example assume minimum \gls{iou}\(=0.5\). Then, in
\cref{fig:bbox-precision-recall} \glspl{bbox} \(A\) and \(B\) would be considered
\glspl{fp} due to their low \gls{iou}, while \gls{bbox} \(B\) is a \gls{tp}.
We count 1 \gls{tp}, 2 \gls{fp} and 1 \gls{fn} and thus observe
\(\text{Precision} = \frac{1}{3}, \text{Recall} = \frac{1}{2}\), following
\cref{eq:precision-recall}.

\begin{figure}[htp!]
    \centering
    \includegraphics[width=.3\textwidth]{metrics3.png}
    \caption{Visual example for bounding-box-wise confusion matrix, with
    groundtruth \glspl{bbox} (red) and inferred \glspl{bbox}(blue). For each
    inferred \gls{bbox} a confidence score (percent) is given.}\label{fig:bbox-precision-recall}
\end{figure}

\paragraph{Precision-Recall-Curves and Average Precision}\label{par:precision-recall-curves-ap}
By ordering set of inferred \glspl{bbox} by confidence and then accumulating
\glspl{tp} and \glspl{fp}, we can construct a Precision-Recall-Curve.
A more extensive example is shown in \cref{fig:precision-recall-curve}.
To improve comparability, a curve \(f(x)\) can be smoothed by applying
\(f(x) = \max (f(x), f(x')), \forall x' > x\). 

\begin{figure}[htp!]
    \begin{subfigure}[t]{.49\linewidth}
        \centering
        \includegraphics[width=.9\textwidth]{rafaelpadilla-precision-recall-curve.png}
        \subcaption{Precision-Recall-Curve. Each capital represents an inferred
        \gls{bbox}. Recall \& Precision are plotted by accumulating \gls{tp} and
        \gls{fp} over the list of inferred \glspl{bbox} sorted by confidence.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.49\linewidth}
        \centering
        \includegraphics[width=.9\textwidth]{rafaelpadilla-average-precision.png}
        \subcaption{Interpolated Precision-Recall-Curve with drawn-in area under the
        curve, i.e.\ \gls{ap}.}
    \end{subfigure}
    \caption[Precision-Recall-Curve]{Precision-Recall-Curve and its
    interpolation. Both figures where adopted from~\cite{Padilla.2019}.}\label{fig:precision-recall-curve}
\end{figure}

\Gls{map} builds upon \gls{ap}. \Gls{ap} is a 
Despite its extensive use in the object detection community~\cite{Liu.2016,Ren.2015}, 
\Gls{map} was only applied in a single work \cite{Zhu.2006} from 2006. This is,
because 
\begin{enumerate*}[i.)]
    \item usually ranks are not generated in most approaches, therefore the 
    underlying metric of average precision, which \textit{is} computed over ranks, 
    lacks meaning and
    \item oftentimes stamp detection is framed as a binary problem.
\end{enumerate*} 

\subsubsection{Normalized Intersection Over Union}\label{subsect:normalized-iou}
\blindtext[1]
\todo{This kind of has the same issues as accuracy. Imagine, 99\% of images are
in the lower left corner, the a System that learns this would have a high 
average IoU. Maybe combine this from ideas of the F-Score.}

\subsection{StaVer}\label{sect:staver}
``The presented method is evaluated on a publicly available dataset (StaVer1) for stamp
detection and verification [sic!]. This dataset contains 400 scanned document images. Out
of these 400 documents, 80 documents contain black stamps whereas the remaining 320
documents contain colored stamps. All of these document images are available in 200,
300, and 600 dpi. For each image, two different types of ground truths are available.
One contains the pixel level ground truth, meaning that all pixels which belong to
stamps are marked in the image. The other ground truth format contains \gls{bbox}
information per stamp. Hence, this dataset can be used for both pixel level and
patch level evaluation of stamp detection. In addition, it contains different types of
stamps ranging from rectangular, oval, to irregular shaped, and most importantly, textual
stamps.
For evaluation of the presented approach, the training set is generated by using 36 documents
out of 400. Out of these 36 training documents, only 6 contains black stamps
whereas the remaining 30 are with colored stamps. Testing is performed on the remaining
364 documents (74 documents with black stamps, 290 documents with colored stamps).
All the results are reported for 200 dpi documents.''~\cite{Ahmed.2016}

\subsection{Results \& Discussion}\label{sect:results-and-discussion}
Goals: 
\begin{enumerate}
    \item How good can stamp detection get with current frameworks?
    \item How good can classification of a vast variety of similar classes get
    with current frameworks?
\end{enumerate}